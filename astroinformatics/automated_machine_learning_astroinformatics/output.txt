MATLAB is selecting SOFTWARE OPENGL rendering.

                            < M A T L A B (R) >
                  Copyright 1984-2016 The MathWorks, Inc.
                   R2016b (9.1.0.441655) 64-bit (glnxa64)
                             September 7, 2016

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 
>> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> Loading data ..
>> >> >> >> >> >> >> >> >> >> >> >> >> >> >> Performing data munging ..
>> >> >> >> >> >> >> >> Training random forests ..
>> 
iNumTrees =

   100

>> >> >> >> >> Regression using random forests ..
>> [Warning: MATLAB has disabled some advanced graphics rendering features by
switching to software OpenGL. For more information, click <a
href="matlab:opengl('problems')">here</a>.] 

min_leaf_size =

     5


BaggedEnsemble = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:           regression
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    5
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    0
                     Proximity:                   []


ans =

         0
    0.8219
         0
    0.8934
    0.0053
    1.0000
    0.3006
         0
    0.0383
         0
         0
         0
    0.4908
    0.0055
    0.9957
    0.6535
         0
         0
    0.9869
    0.0081
    0.7554
    0.9849
         0
    0.6200
         0
    0.6399
         0
         0
         0
         0
         0
         0
    0.9814
         0
         0
         0
    0.9577
    0.1135
    0.9237
    0.8758
    0.9110
    0.7524
    0.9854
    0.5368
    0.9741
    0.0250
    0.9452
         0
    0.0222
    0.9053
    0.7158
    0.5813
    0.8639
    0.9874
    0.9757
    0.9844
    0.8941
    0.6607
    0.7887
    0.7760
    0.7117
         0
    0.9124
    0.4157
         0
    0.5106
         0
    0.8032
    0.9662
         0
         0
    0.7319
    0.2026
    0.9237
    0.8731
    0.0754
    0.0559
         0
    0.0201
         0
    0.0817
         0
         0
    0.1402
    0.9968
    0.8551
    0.8184
    1.0000
         0
    0.0045
         0
         0
         0
         0
         0
         0
         0
         0
    0.9629
    1.0000
    0.8480
    0.6236
         0
    0.5606
    0.8949
    0.6828
    0.9698
    0.9654
    0.0158
    0.9162
    0.2207
    0.8850
    0.9057
    0.9911
    0.1672
    0.0917
    0.3160
    0.9726
    0.0361
         0
         0
         0
    1.0000
    0.7924
    0.8703
         0
    0.9914
         0
    0.8864
    0.1233
         0
         0
    0.8560
         0
         0
    0.0886
         0
         0
         0
         0
         0
         0
    0.9548
    0.1055
         0
         0
    0.7075
         0
    0.3259
    0.9938
         0
    0.9357
    1.0000
    0.0687
         0
    0.7618
         0
         0
         0
    1.0000
    1.0000
         0
         0
    0.0779
         0
         0
    0.9280
    0.8695
         0
    0.0032
         0
         0
         0
    1.0000
    0.9836
    1.0000
         0
    0.9066
         0
    0.4959
         0
         0
    0.0085
    0.9927
    0.9960
         0
    0.9398
    1.0000
    0.9587
    0.0219
    0.9937
    1.0000
         0
         0
         0
         0
    0.9923
    0.0638
         0
         0
    0.9961
    1.0000
         0
         0
         0
    1.0000
    0.9637
         0
         0
         0
    1.0000
    0.9804
         0
    1.0000
    1.0000
    0.9838
         0
         0
    0.9737
    0.0630
         0
    0.9583
    0.0489
    0.9874
    1.0000
         0
    1.0000
    0.9876
         0
    1.0000
    0.8757
    0.9515
    0.8993
    0.9560
    0.1710
    0.1583
    0.9900
    0.9235
    0.9907
    0.9369
    0.1489
    0.0068
         0
    0.2995
         0
         0
    1.0000
    0.8846
    0.1728
         0
         0
    0.9739
    0.8550
    0.9827
    0.9436
    0.8771
         0
         0
         0
    0.9242
    0.9868
    1.0000
    0.9898
    0.9528
    0.9659
    0.1925
    1.0000
    0.8750
    1.0000
         0
    0.9328
         0
    0.8338
    0.6273
         0
         0
         0
         0
         0
    0.9599
         0
         0
    0.8792
    0.9697
    0.9570
    1.0000
    1.0000
         0
    0.6201
    0.9741
         0
         0
    0.8475
    0.8373
         0
    1.0000
    0.8814
    0.3536
    0.3197
    0.6775
    0.8935
         0
    1.0000
         0
    0.8381
    0.9831
         0
         0
    0.9732
    0.2603
         0
         0
    0.7802
         0
         0
    0.8193
    0.9114
    0.9418
    0.0913
    0.8413
    0.9888
         0
         0
    0.9254
         0
    0.0392
    0.5724
         0
    0.8149
    0.9212
    0.9480
    0.0247
         0
    0.9233
    0.9919
         0
    0.9295
         0
         0
    0.9892
    0.9133
         0
         0
         0
    0.9914
         0
    0.0932
    0.0120
    0.5519
    0.7080
    0.0024
         0
    0.6941
    0.9649
         0
         0
    0.3490
    1.0000
    0.9750
    0.8884
    1.0000
    0.9179
    0.0578
    0.4495
         0
         0
    1.0000
    1.0000
         0
         0
    0.0056
         0
         0
         0
         0
         0
         0
         0
    0.0299
    0.2892
         0
    0.9771
         0
         0
         0
         0
    0.9694
    0.1474
         0
    0.0027
         0
    1.0000
         0
         0
         0
         0
         0
         0
         0
    0.0046
    0.9965
         0
    0.0236
    0.0361
    0.0026
         0
         0
         0
    0.0404
         0
         0
         0
    0.9893
         0
    1.0000
    0.4665
    0.9705
         0
    0.0729
    0.0639
    0.0330
    0.9763
    0.2190
         0
         0
    1.0000
    0.2582
    0.9830
         0
         0
         0
    0.0228
         0
         0
    0.9879
    0.9807
    0.8779
         0
    0.0082
         0
    0.8775
    0.1466
    0.0153
         0
    0.0472
         0
         0
         0
         0
    1.0000
    0.0227
         0
         0
    0.9706
    0.0291
    0.4245
    1.0000
    0.9695
         0
         0
    0.0048
    0.0797
    0.0065
         0
         0
    0.9948
    1.0000
    0.9544
         0
         0
         0
    0.0029
    0.0030
         0
         0
         0
         0
         0
         0
    0.9967
         0
    0.0876
    1.0000
    0.9958
         0
    0.0155
         0
    1.0000
    0.7387
    0.4366
         0
    0.9809
    0.0038
    1.0000
    0.2072
         0
         0
         0
         0
         0
    0.0119
         0
         0
         0
         0
         0
    0.9690
    0.0307
         0
         0
         0
         0
         0
         0
    1.0000
    0.9714
         0
         0
         0
    0.9516
         0
         0
    0.8983
    0.9889
         0
         0
         0
         0
    0.0053
    0.0022
    0.8666
         0
         0
         0
         0
    0.0335
         0
         0
         0
         0
         0
         0
    0.0359
         0
         0
         0
    0.9957
         0
         0
    0.9233
         0
         0
    0.1065
    0.4341
         0
    0.3055
         0
    0.0011
         0
         0
         0
         0
         0
         0
         0
    1.0000
         0
    0.0763
    0.8307
    0.9907
    0.9882
    0.9958
         0
         0
    0.9943
         0
         0
         0
         0
         0
         0
    1.0000
    0.9954
         0
    0.0444
         0
    1.0000
         0
    0.8945
         0
    0.8389
    0.9898
    0.8740
         0
    0.9732
         0
         0
    0.0403
         0
    0.0642
         0
         0
         0
    0.6807
    0.6733
    0.9749
         0
         0
    0.9938
         0
    0.8922
    0.9384
    1.0000
         0
         0
         0
         0
         0
         0
         0
         0
    0.4258
    0.2108
         0
    0.0106
    0.1998
    0.9688
    0.0821
         0
         0
    0.0312
         0
         0
    0.9148
         0
    0.0609
    0.9706
    0.1781
         0
    0.0024
    0.0038
         0
         0
         0
         0
         0
         0
    0.0013
    0.9564
         0
    0.0230
    0.0035
         0
         0
         0
         0
         0
    0.6790
    0.9737
         0
         0
         0
         0
         0
    0.0055
         0
         0
         0
    0.6933
    0.9910
    0.9810
         0
         0
    0.0223
         0
    0.0355
         0
         0
         0
         0
    1.0000
    0.9946
         0
         0
         0
         0
         0
    0.0233
         0
    0.0565
    0.0031
    0.8894
         0
    0.0419
         0
         0
    0.9719
    0.9213
    0.9650


Decision tree for regression
 1  if x3<3.5 then node 2 elseif x3>=3.5 then node 3 else 0.373391
 2  if x2<2.5 then node 4 elseif x2>=2.5 then node 5 else 0.0755556
 3  if x2<4.5 then node 6 elseif x2>=4.5 then node 7 else 0.911647
 4  if x6<4.5 then node 8 elseif x6>=4.5 then node 9 else 0.0222222
 5  if x5<3.5 then node 10 elseif x5>=3.5 then node 11 else 0.555556
 6  if x8<2.5 then node 12 elseif x8>=2.5 then node 13 else 0.742424
 7  if x4<1.5 then node 14 elseif x4>=1.5 then node 15 else 0.972678
 8  if x5<3.5 then node 16 elseif x5>=3.5 then node 17 else 0.00507614
 9  if x3<1.5 then node 18 elseif x3>=1.5 then node 19 else 0.636364
10  if x2<3.5 then node 20 elseif x2>=3.5 then node 21 else 0.321429
11  if x4<5.5 then node 22 elseif x4>=5.5 then node 23 else 0.941176
12  if x6<5.5 then node 24 elseif x6>=5.5 then node 25 else 0.421053
13  if x5<2.5 then node 26 elseif x5>=2.5 then node 27 else 0.87234
14  if x3<9 then node 28 elseif x3>=9 then node 29 else 0.75
15  if x6<1.5 then node 30 elseif x6>=1.5 then node 31 else 0.988304
16  fit = 0
17  fit = 0.25
18  fit = 0.4
19  fit = 0.833333
20  if x7<2.5 then node 32 elseif x7>=2.5 then node 33 else 0.142857
21  fit = 0.857143
22  fit = 0.8
23  fit = 1
24  if x6<0.5 then node 34 elseif x6>=0.5 then node 35 else 0.0909091
25  fit = 0.875
26  fit = 1
27  if x4<2 then node 36 elseif x4>=2 then node 37 else 0.8125
28  fit = 0.571429
29  fit = 1
30  if x3<7.5 then node 38 elseif x3>=7.5 then node 39 else 0.857143
31  fit = 1
32  fit = 0
33  if x6<2.5 then node 40 elseif x6>=2.5 then node 41 else 0.214286
34  fit = 0
35  if x2<2.5 then node 42 elseif x2>=2.5 then node 43 else 0.1
36  fit = 1
37  if x3<4.5 then node 44 elseif x3>=4.5 then node 45 else 0.666667
38  fit = 0.666667
39  fit = 1
40  fit = 0
41  fit = 0.375
42  fit = 0
43  fit = 0.166667
44  fit = 0.5
45  if x7<5.5 then node 46 elseif x7>=5.5 then node 47 else 0.75
46  fit = 0.4
47  fit = 1


b = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:           regression
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    5
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    1
                     Proximity:                   []


idxvar =

     1     2     3     6     7


BaggedEnsemble = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:           regression
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    5
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    0
                     Proximity:                   []

>> >> Performing prediction ..
>> 
ans =

     0

>> >> Classification using random forests ..
>> 
min_leaf_size =

     5


BaggedEnsemble = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:       classification
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    1
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    0
                     Proximity:                   []
                    ClassNames:             '0'             '1'


ans =

  699×1 cell array

    '0'
    '1'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '1'
    '1'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '1'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '1'
    '0'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '1'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '0'
    '1'
    '0'
    '0'
    '0'
    '0'
    '1'
    '1'
    '1'


Decision tree for classification
 1  if x5<2.5 then node 2 elseif x5>=2.5 then node 3 else 0
 2  if x6<5.5 then node 4 elseif x6>=5.5 then node 5 else 0
 3  if x3<1.5 then node 6 elseif x3>=1.5 then node 7 else 1
 4  if x4<3.5 then node 8 elseif x4>=3.5 then node 9 else 0
 5  if x7<3.5 then node 10 elseif x7>=3.5 then node 11 else 1
 6  class = 0
 7  if x6<0.5 then node 12 elseif x6>=0.5 then node 13 else 1
 8  if x1<7 then node 14 elseif x1>=7 then node 15 else 0
 9  if x3<2.5 then node 16 elseif x3>=2.5 then node 17 else 1
10  if x1<3 then node 18 elseif x1>=3 then node 19 else 1
11  class = 1
12  class = 0
13  if x4<5.5 then node 20 elseif x4>=5.5 then node 21 else 1
14  if x6<2.5 then node 22 elseif x6>=2.5 then node 23 else 0
15  if x2<3.5 then node 24 elseif x2>=3.5 then node 25 else 1
16  class = 0
17  class = 1
18  class = 0
19  class = 1
20  if x2<4.5 then node 26 elseif x2>=4.5 then node 27 else 1
21  class = 1
22  class = 0
23  if x5<1.5 then node 28 elseif x5>=1.5 then node 29 else 0
24  class = 0
25  class = 1
26  if x7<2.5 then node 30 elseif x7>=2.5 then node 31 else 1
27  class = 1
28  class = 1
29  if x2<2 then node 32 elseif x2>=2 then node 33 else 0
30  class = 0
31  if x8<8.5 then node 34 elseif x8>=8.5 then node 35 else 1
32  class = 0
33  if x8<2.5 then node 36 elseif x8>=2.5 then node 37 else 0
34  if x8<4.5 then node 38 elseif x8>=4.5 then node 39 else 1
35  class = 1
36  class = 0
37  class = 1
38  if x1<6.5 then node 40 elseif x1>=6.5 then node 41 else 1
39  if x5<3.5 then node 42 elseif x5>=3.5 then node 43 else 0
40  if x6<2.5 then node 44 elseif x6>=2.5 then node 45 else 1
41  class = 1
42  if x8<5.5 then node 46 elseif x8>=5.5 then node 47 else 1
43  class = 0
44  class = 0
45  if x8<2 then node 48 elseif x8>=2 then node 49 else 1
46  if x1<8 then node 50 elseif x1>=8 then node 51 else 0
47  class = 1
48  class = 1
49  if x4<2.5 then node 52 elseif x4>=2.5 then node 53 else 1
50  class = 0
51  class = 1
52  class = 1
53  class = 0


b = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:       classification
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    5
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    1
                     Proximity:                   []
                    ClassNames:             '0'             '1'


idxvar =

     1     2     3     6     7


BaggedEnsemble = 

  TreeBagger
Ensemble with 100 bagged decision trees:
                    Training X:              [699x9]
                    Training Y:              [699x1]
                        Method:       classification
                 NumPredictors:                    9
         NumPredictorsToSample:                    3
                   MinLeafSize:                    1
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    0
                     Proximity:                   []
                    ClassNames:             '0'             '1'

>> >> Performing prediction ..
>> 
ans =

  cell

    '0'

>> >> >> >> >> Performing PCA ..
>> >> >> >> 
extreme =

    72
   168
    86
    99
   329

Most extreme points:

ans =

     1123061
     1198128
     1147748
     1165926
      760001

gs: /opt/matlab/bin/glnxa64/libtiff.so.5: no version information available (required by /usr/lib/x86_64-linux-gnu/libgs.so.9)

ans =

     0

>> >> >> >> >> >> >> >> >> >> 
i_num_data_columns =

     9

>> >> >> >> >> >> >> Performing prediction using neural networks ..
>> >> >> 
parallel =

     0

>> >> 
iNumHidden =

     1

Training neural network ..
Starting parallel pool (parpool) using the 'local' profile ... connected to 2 workers.

perf =

    0.0345

Predicting using neural network ..
Elapsed time is 27.234169 seconds.

iNumHidden =

     2

Training neural network ..

perf =

    0.0116

Predicting using neural network ..
Elapsed time is 2.686076 seconds.

iNumHidden =

     3

Training neural network ..

perf =

    0.0423

Predicting using neural network ..
Elapsed time is 2.080847 seconds.

iNumHidden =

     4

Training neural network ..

perf =

    0.0467

Predicting using neural network ..
Elapsed time is 1.864187 seconds.

iNumHidden =

     5

Training neural network ..

perf =

    0.0190

Predicting using neural network ..
Elapsed time is 1.917100 seconds.

iNumHidden =

    10

Training neural network ..

perf =

    0.0108

Predicting using neural network ..
Elapsed time is 1.730127 seconds.

iNumHidden =

    20

Training neural network ..

perf =

    0.0274

Predicting using neural network ..
Elapsed time is 1.802501 seconds.

iNumHidden =

    30

Training neural network ..

perf =

    0.0421

Predicting using neural network ..
Elapsed time is 1.910162 seconds.

iNumHidden =

    50

Training neural network ..

perf =

    0.0332

Predicting using neural network ..
Elapsed time is 2.258495 seconds.

iNumHidden =

   100

Training neural network ..

perf =

    0.0631

Predicting using neural network ..
Elapsed time is 4.431922 seconds.
>> >> >> 
iNumHidden =

    10

>> Training neural network ..

perf =

    0.0280

Predicting using neural network ..
Elapsed time is 1.703145 seconds.
>> >> >> >> >> >> >> >> >> Performing LASSO on logistic regression ..

B =

  Columns 1 through 7

    0.5359    0.5358    0.5356    0.5355    0.5354    0.5352    0.5350
    0.0127    0.0127    0.0128    0.0128    0.0129    0.0130    0.0131
    0.3249    0.3248    0.3248    0.3247    0.3246    0.3246    0.3245
    0.2375    0.2374    0.2373    0.2372    0.2370    0.2369    0.2367
    0.0542    0.0542    0.0542    0.0542    0.0542    0.0541    0.0541
    0.4281    0.4281    0.4280    0.4279    0.4278    0.4278    0.4277
    0.4171    0.4170    0.4169    0.4168    0.4166    0.4165    0.4163
    0.1634    0.1634    0.1634    0.1633    0.1633    0.1633    0.1632
    0.5251    0.5246    0.5241    0.5236    0.5230    0.5224    0.5216

  Columns 8 through 14

    0.5348    0.5346    0.5343    0.5341    0.5338    0.5334    0.5331
    0.0131    0.0132    0.0134    0.0135    0.0136    0.0138    0.0139
    0.3244    0.3243    0.3242    0.3241    0.3240    0.3239    0.3237
    0.2366    0.2364    0.2362    0.2359    0.2357    0.2354    0.2351
    0.0541    0.0541    0.0541    0.0540    0.0540    0.0540    0.0540
    0.4276    0.4274    0.4273    0.4272    0.4270    0.4269    0.4267
    0.4161    0.4160    0.4157    0.4155    0.4153    0.4150    0.4147
    0.1632    0.1631    0.1631    0.1630    0.1629    0.1628    0.1628
    0.5209    0.5200    0.5191    0.5181    0.5169    0.5157    0.5144

  Columns 15 through 21

    0.5327    0.5323    0.5318    0.5313    0.5308    0.5302    0.5295
    0.0141    0.0143    0.0145    0.0147    0.0150    0.0152    0.0155
    0.3236    0.3234    0.3232    0.3230    0.3228    0.3226    0.3223
    0.2348    0.2345    0.2341    0.2337    0.2332    0.2327    0.2322
    0.0539    0.0539    0.0539    0.0538    0.0538    0.0537    0.0537
    0.4265    0.4263    0.4260    0.4258    0.4255    0.4252    0.4248
    0.4143    0.4140    0.4136    0.4132    0.4127    0.4122    0.4116
    0.1627    0.1626    0.1625    0.1623    0.1622    0.1621    0.1619
    0.5129    0.5113    0.5095    0.5076    0.5055    0.5032    0.5007

  Columns 22 through 28

    0.5288    0.5281    0.5272    0.5263    0.5253    0.5242    0.5231
    0.0159    0.0162    0.0166    0.0170    0.0175    0.0180    0.0186
    0.3220    0.3217    0.3214    0.3210    0.3206    0.3201    0.3197
    0.2316    0.2309    0.2302    0.2294    0.2286    0.2277    0.2267
    0.0536    0.0536    0.0535    0.0534    0.0533    0.0532    0.0532
    0.4245    0.4241    0.4236    0.4231    0.4226    0.4221    0.4214
    0.4110    0.4103    0.4096    0.4088    0.4079    0.4070    0.4059
    0.1617    0.1615    0.1613    0.1611    0.1609    0.1606    0.1603
    0.4980    0.4951    0.4918    0.4883    0.4845    0.4803    0.4758

  Columns 29 through 35

    0.5218    0.5204    0.5189    0.5173    0.5156    0.5137    0.5117
    0.0191    0.0198    0.0205    0.0213    0.0221    0.0230    0.0240
    0.3191    0.3186    0.3179    0.3172    0.3165    0.3157    0.3148
    0.2256    0.2244    0.2232    0.2218    0.2203    0.2187    0.2169
    0.0531    0.0529    0.0528    0.0527    0.0526    0.0524    0.0523
    0.4208    0.4200    0.4192    0.4183    0.4174    0.4164    0.4153
    0.4048    0.4036    0.4023    0.4008    0.3993    0.3976    0.3958
    0.1600    0.1596    0.1592    0.1588    0.1584    0.1579    0.1574
    0.4710    0.4657    0.4600    0.4538    0.4472    0.4401    0.4325

  Columns 36 through 42

    0.5095    0.5072    0.5046    0.5019    0.4991    0.4960    0.4927
    0.0251    0.0262    0.0275    0.0289    0.0303    0.0319    0.0336
    0.3139    0.3128    0.3117    0.3105    0.3092    0.3078    0.3062
    0.2150    0.2130    0.2108    0.2085    0.2059    0.2033    0.2004
    0.0521    0.0519    0.0517    0.0515    0.0513    0.0511    0.0508
    0.4141    0.4128    0.4114    0.4099    0.4083    0.4066    0.4047
    0.3938    0.3917    0.3894    0.3870    0.3844    0.3816    0.3785
    0.1568    0.1562    0.1555    0.1548    0.1540    0.1532    0.1523
    0.4244    0.4157    0.4064    0.3967    0.3863    0.3754    0.3640

  Columns 43 through 49

    0.4892    0.4854    0.4814    0.4772    0.4727    0.4679    0.4629
    0.0354    0.0374    0.0395    0.0418    0.0443    0.0469    0.0497
    0.3046    0.3028    0.3008    0.2987    0.2964    0.2939    0.2912
    0.1973    0.1941    0.1906    0.1869    0.1830    0.1789    0.1746
    0.0506    0.0503    0.0500    0.0497    0.0493    0.0489    0.0485
    0.4027    0.4006    0.3983    0.3959    0.3934    0.3907    0.3878
    0.3753    0.3719    0.3682    0.3643    0.3602    0.3558    0.3511
    0.1513    0.1502    0.1491    0.1479    0.1467    0.1453    0.1439
    0.3521    0.3397    0.3268    0.3135    0.2999    0.2859    0.2716

  Columns 50 through 56

    0.4575    0.4519    0.4460    0.4398    0.4333    0.4265    0.4194
    0.0527    0.0560    0.0594    0.0631    0.0670    0.0711    0.0755
    0.2883    0.2852    0.2818    0.2781    0.2742    0.2700    0.2656
    0.1701    0.1653    0.1604    0.1552    0.1498    0.1442    0.1385
    0.0481    0.0476    0.0472    0.0466    0.0461    0.0455    0.0448
    0.3847    0.3816    0.3782    0.3747    0.3710    0.3672    0.3632
    0.3462    0.3411    0.3357    0.3300    0.3241    0.3179    0.3114
    0.1423    0.1407    0.1390    0.1373    0.1354    0.1335    0.1314
    0.2571    0.2423    0.2275    0.2124    0.1973    0.1822    0.1670

  Columns 57 through 63

    0.4120    0.4043    0.3963    0.3880    0.3795    0.3707    0.3616
    0.0801    0.0849    0.0900    0.0952    0.1006    0.1062    0.1119
    0.2608    0.2558    0.2504    0.2449    0.2391    0.2330    0.2268
    0.1325    0.1264    0.1202    0.1139    0.1075    0.1011    0.0946
    0.0441    0.0434    0.0426    0.0418    0.0409    0.0400    0.0390
    0.3590    0.3547    0.3503    0.3457    0.3411    0.3362    0.3313
    0.3047    0.2978    0.2906    0.2832    0.2756    0.2678    0.2598
    0.1293    0.1271    0.1249    0.1225    0.1201    0.1176    0.1151
    0.1518    0.1366    0.1215    0.1064    0.0915    0.0766    0.0619

  Columns 64 through 70

    0.3522    0.3426    0.3327    0.3226    0.3109    0.2982    0.2854
    0.1176    0.1234    0.1292    0.1348    0.1386    0.1411    0.1436
    0.2205    0.2141    0.2077    0.2013    0.1962    0.1918    0.1874
    0.0881    0.0817    0.0754    0.0692    0.0630    0.0569    0.0509
    0.0379    0.0368    0.0357    0.0344    0.0321    0.0289    0.0254
    0.3263    0.3212    0.3159    0.3106    0.3052    0.2999    0.2944
    0.2516    0.2432    0.2346    0.2259    0.2173    0.2087    0.2001
    0.1125    0.1097    0.1070    0.1041    0.1011    0.0981    0.0950
    0.0474    0.0331    0.0190    0.0052         0         0         0

  Columns 71 through 77

    0.2727    0.2600    0.2474    0.2348    0.2222    0.2098    0.1974
    0.1460    0.1483    0.1506    0.1527    0.1547    0.1563    0.1564
    0.1828    0.1782    0.1736    0.1689    0.1643    0.1597    0.1551
    0.0449    0.0391    0.0333    0.0276    0.0220    0.0164    0.0104
    0.0217    0.0177    0.0135    0.0090    0.0043         0         0
    0.2890    0.2835    0.2779    0.2724    0.2668    0.2611    0.2552
    0.1914    0.1827    0.1740    0.1654    0.1568    0.1483    0.1399
    0.0918    0.0885    0.0852    0.0817    0.0782    0.0745    0.0699
         0         0         0         0         0         0         0

  Columns 78 through 84

    0.1851    0.1729    0.1611    0.1493    0.1376    0.1259    0.1141
    0.1562    0.1554    0.1528    0.1501    0.1473    0.1444    0.1415
    0.1505    0.1459    0.1413    0.1367    0.1323    0.1279    0.1236
    0.0045         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.2492    0.2429    0.2355    0.2280    0.2203    0.2126    0.2048
    0.1316    0.1232    0.1139    0.1048    0.0958    0.0869    0.0781
    0.0653    0.0604    0.0552    0.0498    0.0443    0.0386    0.0327
         0         0         0         0         0         0         0

  Columns 85 through 91

    0.1023    0.0905    0.0785    0.0664    0.0540    0.0409    0.0274
    0.1385    0.1355    0.1324    0.1293    0.1262    0.1211    0.1159
    0.1195    0.1155    0.1116    0.1080    0.1045    0.0991    0.0939
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.1968    0.1888    0.1806    0.1722    0.1637    0.1550    0.1460
    0.0694    0.0607    0.0521    0.0435    0.0349    0.0240    0.0129
    0.0267    0.0204    0.0138    0.0070         0         0         0
         0         0         0         0         0         0         0

  Columns 92 through 98

    0.0134         0         0         0         0         0         0
    0.1107    0.1017    0.0889    0.0757    0.0619    0.0475    0.0321
    0.0889    0.0824    0.0722    0.0619    0.0514    0.0407    0.0296
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.1369    0.1253    0.1105    0.0950    0.0785    0.0611    0.0423
    0.0015         0         0         0         0         0         0
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0

  Columns 99 through 100

         0         0
    0.0152         0
    0.0176         0
         0         0
         0         0
    0.0217         0
         0         0
         0         0
         0         0


FitInfo = 

  struct with fields:

            Intercept: [1×100 double]
               Lambda: [1×100 double]
                Alpha: 1
                   DF: [1×100 double]
             Deviance: [1×100 double]
       PredictorNames: {}
                   SE: [1×100 double]
    LambdaMinDeviance: 0.0018
            Lambda1SE: 0.0125
     IndexMinDeviance: 42
             Index1SE: 63


indx =

    42

Number of non-zero predictors after LASSO

nonzero_predictors =

     9

Performing prediction ..

B =

  Columns 1 through 7

    0.5359    0.5358    0.5356    0.5355    0.5354    0.5352    0.5350
    0.0127    0.0127    0.0128    0.0128    0.0129    0.0130    0.0131
    0.3249    0.3248    0.3248    0.3247    0.3246    0.3246    0.3245
    0.2375    0.2374    0.2373    0.2372    0.2370    0.2369    0.2367
    0.0542    0.0542    0.0542    0.0542    0.0542    0.0541    0.0541
    0.4281    0.4281    0.4280    0.4279    0.4278    0.4278    0.4277
    0.4171    0.4170    0.4169    0.4168    0.4166    0.4165    0.4163
    0.1634    0.1634    0.1634    0.1633    0.1633    0.1633    0.1632
    0.5251    0.5246    0.5241    0.5236    0.5230    0.5224    0.5216

  Columns 8 through 14

    0.5348    0.5346    0.5343    0.5341    0.5338    0.5334    0.5331
    0.0131    0.0132    0.0134    0.0135    0.0136    0.0138    0.0139
    0.3244    0.3243    0.3242    0.3241    0.3240    0.3239    0.3237
    0.2366    0.2364    0.2362    0.2359    0.2357    0.2354    0.2351
    0.0541    0.0541    0.0541    0.0540    0.0540    0.0540    0.0540
    0.4276    0.4274    0.4273    0.4272    0.4270    0.4269    0.4267
    0.4161    0.4160    0.4157    0.4155    0.4153    0.4150    0.4147
    0.1632    0.1631    0.1631    0.1630    0.1629    0.1628    0.1628
    0.5209    0.5200    0.5191    0.5181    0.5169    0.5157    0.5144

  Columns 15 through 21

    0.5327    0.5323    0.5318    0.5313    0.5308    0.5302    0.5295
    0.0141    0.0143    0.0145    0.0147    0.0150    0.0152    0.0155
    0.3236    0.3234    0.3232    0.3230    0.3228    0.3226    0.3223
    0.2348    0.2345    0.2341    0.2337    0.2332    0.2327    0.2322
    0.0539    0.0539    0.0539    0.0538    0.0538    0.0537    0.0537
    0.4265    0.4263    0.4260    0.4258    0.4255    0.4252    0.4248
    0.4143    0.4140    0.4136    0.4132    0.4127    0.4122    0.4116
    0.1627    0.1626    0.1625    0.1623    0.1622    0.1621    0.1619
    0.5129    0.5113    0.5095    0.5076    0.5055    0.5032    0.5007

  Columns 22 through 28

    0.5288    0.5281    0.5272    0.5263    0.5253    0.5242    0.5231
    0.0159    0.0162    0.0166    0.0170    0.0175    0.0180    0.0186
    0.3220    0.3217    0.3214    0.3210    0.3206    0.3201    0.3197
    0.2316    0.2309    0.2302    0.2294    0.2286    0.2277    0.2267
    0.0536    0.0536    0.0535    0.0534    0.0533    0.0532    0.0532
    0.4245    0.4241    0.4236    0.4231    0.4226    0.4221    0.4214
    0.4110    0.4103    0.4096    0.4088    0.4079    0.4070    0.4059
    0.1617    0.1615    0.1613    0.1611    0.1609    0.1606    0.1603
    0.4980    0.4951    0.4918    0.4883    0.4845    0.4803    0.4758

  Columns 29 through 35

    0.5218    0.5204    0.5189    0.5173    0.5156    0.5137    0.5117
    0.0191    0.0198    0.0205    0.0213    0.0221    0.0230    0.0240
    0.3191    0.3186    0.3179    0.3172    0.3165    0.3157    0.3148
    0.2256    0.2244    0.2232    0.2218    0.2203    0.2187    0.2169
    0.0531    0.0529    0.0528    0.0527    0.0526    0.0524    0.0523
    0.4208    0.4200    0.4192    0.4183    0.4174    0.4164    0.4153
    0.4048    0.4036    0.4023    0.4008    0.3993    0.3976    0.3958
    0.1600    0.1596    0.1592    0.1588    0.1584    0.1579    0.1574
    0.4710    0.4657    0.4600    0.4538    0.4472    0.4401    0.4325

  Columns 36 through 42

    0.5095    0.5072    0.5046    0.5019    0.4991    0.4960    0.4927
    0.0251    0.0262    0.0275    0.0289    0.0303    0.0319    0.0336
    0.3139    0.3128    0.3117    0.3105    0.3092    0.3078    0.3062
    0.2150    0.2130    0.2108    0.2085    0.2059    0.2033    0.2004
    0.0521    0.0519    0.0517    0.0515    0.0513    0.0511    0.0508
    0.4141    0.4128    0.4114    0.4099    0.4083    0.4066    0.4047
    0.3938    0.3917    0.3894    0.3870    0.3844    0.3816    0.3785
    0.1568    0.1562    0.1555    0.1548    0.1540    0.1532    0.1523
    0.4244    0.4157    0.4064    0.3967    0.3863    0.3754    0.3640

  Columns 43 through 49

    0.4892    0.4854    0.4814    0.4772    0.4727    0.4679    0.4629
    0.0354    0.0374    0.0395    0.0418    0.0443    0.0469    0.0497
    0.3046    0.3028    0.3008    0.2987    0.2964    0.2939    0.2912
    0.1973    0.1941    0.1906    0.1869    0.1830    0.1789    0.1746
    0.0506    0.0503    0.0500    0.0497    0.0493    0.0489    0.0485
    0.4027    0.4006    0.3983    0.3959    0.3934    0.3907    0.3878
    0.3753    0.3719    0.3682    0.3643    0.3602    0.3558    0.3511
    0.1513    0.1502    0.1491    0.1479    0.1467    0.1453    0.1439
    0.3521    0.3397    0.3268    0.3135    0.2999    0.2859    0.2716

  Columns 50 through 56

    0.4575    0.4519    0.4460    0.4398    0.4333    0.4265    0.4194
    0.0527    0.0560    0.0594    0.0631    0.0670    0.0711    0.0755
    0.2883    0.2852    0.2818    0.2781    0.2742    0.2700    0.2656
    0.1701    0.1653    0.1604    0.1552    0.1498    0.1442    0.1385
    0.0481    0.0476    0.0472    0.0466    0.0461    0.0455    0.0448
    0.3847    0.3816    0.3782    0.3747    0.3710    0.3672    0.3632
    0.3462    0.3411    0.3357    0.3300    0.3241    0.3179    0.3114
    0.1423    0.1407    0.1390    0.1373    0.1354    0.1335    0.1314
    0.2571    0.2423    0.2275    0.2124    0.1973    0.1822    0.1670

  Columns 57 through 63

    0.4120    0.4043    0.3963    0.3880    0.3795    0.3707    0.3616
    0.0801    0.0849    0.0900    0.0952    0.1006    0.1062    0.1119
    0.2608    0.2558    0.2504    0.2449    0.2391    0.2330    0.2268
    0.1325    0.1264    0.1202    0.1139    0.1075    0.1011    0.0946
    0.0441    0.0434    0.0426    0.0418    0.0409    0.0400    0.0390
    0.3590    0.3547    0.3503    0.3457    0.3411    0.3362    0.3313
    0.3047    0.2978    0.2906    0.2832    0.2756    0.2678    0.2598
    0.1293    0.1271    0.1249    0.1225    0.1201    0.1176    0.1151
    0.1518    0.1366    0.1215    0.1064    0.0915    0.0766    0.0619

  Columns 64 through 70

    0.3522    0.3426    0.3327    0.3226    0.3109    0.2982    0.2854
    0.1176    0.1234    0.1292    0.1348    0.1386    0.1411    0.1436
    0.2205    0.2141    0.2077    0.2013    0.1962    0.1918    0.1874
    0.0881    0.0817    0.0754    0.0692    0.0630    0.0569    0.0509
    0.0379    0.0368    0.0357    0.0344    0.0321    0.0289    0.0254
    0.3263    0.3212    0.3159    0.3106    0.3052    0.2999    0.2944
    0.2516    0.2432    0.2346    0.2259    0.2173    0.2087    0.2001
    0.1125    0.1097    0.1070    0.1041    0.1011    0.0981    0.0950
    0.0474    0.0331    0.0190    0.0052         0         0         0

  Columns 71 through 77

    0.2727    0.2600    0.2474    0.2348    0.2222    0.2098    0.1974
    0.1460    0.1483    0.1506    0.1527    0.1547    0.1563    0.1564
    0.1828    0.1782    0.1736    0.1689    0.1643    0.1597    0.1551
    0.0449    0.0391    0.0333    0.0276    0.0220    0.0164    0.0104
    0.0217    0.0177    0.0135    0.0090    0.0043         0         0
    0.2890    0.2835    0.2779    0.2724    0.2668    0.2611    0.2552
    0.1914    0.1827    0.1740    0.1654    0.1568    0.1483    0.1399
    0.0918    0.0885    0.0852    0.0817    0.0782    0.0745    0.0699
         0         0         0         0         0         0         0

  Columns 78 through 84

    0.1851    0.1729    0.1611    0.1493    0.1376    0.1259    0.1141
    0.1562    0.1554    0.1528    0.1501    0.1473    0.1444    0.1415
    0.1505    0.1459    0.1413    0.1367    0.1323    0.1279    0.1236
    0.0045         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.2492    0.2429    0.2355    0.2280    0.2203    0.2126    0.2048
    0.1316    0.1232    0.1139    0.1048    0.0958    0.0869    0.0781
    0.0653    0.0604    0.0552    0.0498    0.0443    0.0386    0.0327
         0         0         0         0         0         0         0

  Columns 85 through 91

    0.1023    0.0905    0.0785    0.0664    0.0540    0.0409    0.0274
    0.1385    0.1355    0.1324    0.1293    0.1262    0.1211    0.1159
    0.1195    0.1155    0.1116    0.1080    0.1045    0.0991    0.0939
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.1968    0.1888    0.1806    0.1722    0.1637    0.1550    0.1460
    0.0694    0.0607    0.0521    0.0435    0.0349    0.0240    0.0129
    0.0267    0.0204    0.0138    0.0070         0         0         0
         0         0         0         0         0         0         0

  Columns 92 through 98

    0.0134         0         0         0         0         0         0
    0.1107    0.1017    0.0889    0.0757    0.0619    0.0475    0.0321
    0.0889    0.0824    0.0722    0.0619    0.0514    0.0407    0.0296
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0
    0.1369    0.1253    0.1105    0.0950    0.0785    0.0611    0.0423
    0.0015         0         0         0         0         0         0
         0         0         0         0         0         0         0
         0         0         0         0         0         0         0

  Columns 99 through 100

         0         0
    0.0152         0
    0.0176         0
         0         0
         0         0
    0.0217         0
         0         0
         0         0
         0         0


preds =

    0.0230
    0.8928
    0.0130
    0.7858
    0.0210
    0.9999
    0.1112
    0.0072
    0.0107
    0.0101
    0.0031
    0.0037
    0.2641
    0.0073
    0.9990
    0.5878
    0.0097
    0.0142
    0.9981
    0.0371
    0.9960
    0.9975
    0.0060
    0.6148
    0.0033
    0.6652
    0.0059
    0.0158
    0.0037
    0.0028
    0.0057
    0.0053
    0.9969
    0.0065
    0.0081
    0.0037
    0.9985
    0.1467
    0.9826
    0.6625
    0.9186
    0.9165
    0.9985
    0.2792
    0.9974
    0.0032
    0.9363
    0.0022
    0.0210
    0.9796
    0.9157
    0.3104
    0.9865
    0.9981
    0.9968
    0.9643
    0.9834
    0.7574
    0.8174
    0.7195
    0.7217
    0.0034
    0.9920
    0.3784
    0.0022
    0.9717
    0.0142
    0.9416
    0.9999
    0.0038
    0.0289
    0.9956
    0.0401
    0.9987
    0.8749
    0.0112
    0.0056
    0.0206
    0.0193
    0.0039
    0.0315
    0.0119
    0.0237
    0.0564
    0.9993
    0.9912
    0.7833
    0.9909
    0.0142
    0.0047
    0.0033
    0.0050
    0.0142
    0.0022
    0.0053
    0.0033
    0.0031
    0.0230
    0.9995
    0.9998
    0.9774
    0.1429
    0.0191
    0.7111
    0.9999
    0.8828
    0.9993
    0.9822
    0.0030
    0.9933
    0.0260
    0.9312
    0.9994
    0.9993
    0.0279
    0.0077
    0.2620
    0.9993
    0.0038
    0.0134
    0.0054
    0.0218
    0.9996
    0.9276
    0.9941
    0.0022
    0.9985
    0.0087
    0.9778
    0.0023
    0.0289
    0.0053
    0.9974
    0.0070
    0.0063
    0.0456
    0.0097
    0.0041
    0.0132
    0.0014
    0.0041
    0.0025
    0.9649
    0.0077
    0.0037
    0.0028
    0.5512
    0.0035
    0.0991
    0.9999
    0.0031
    0.9816
    0.9992
    0.0149
    0.0015
    0.9092
    0.0031
    0.0053
    0.0015
    1.0000
    0.9992
    0.0165
    0.0087
    0.0344
    0.0154
    0.0245
    0.9967
    0.9991
    0.0087
    0.0018
    0.0041
    0.0033
    0.0022
    1.0000
    0.9896
    0.9948
    0.0053
    0.9463
    0.0142
    0.7642
    0.0031
    0.0015
    0.0371
    0.9984
    0.9897
    0.0051
    0.9686
    1.0000
    0.9971
    0.0062
    0.9999
    0.9998
    0.0158
    0.0033
    0.0087
    0.0142
    0.9935
    0.0411
    0.0015
    0.0060
    0.9997
    0.9995
    0.0033
    0.0230
    0.0033
    0.9999
    0.9872
    0.0031
    0.0031
    0.0219
    1.0000
    0.9990
    0.0033
    1.0000
    1.0000
    0.9961
    0.0022
    0.0033
    0.9941
    0.0663
    0.0040
    0.9996
    0.0658
    0.9846
    0.9996
    0.0022
    0.9997
    0.9953
    0.0031
    0.9999
    0.9893
    0.9978
    0.6212
    0.9934
    0.0279
    0.0145
    1.0000
    0.9922
    1.0000
    0.9914
    0.0827
    0.0123
    0.0053
    0.0341
    0.0033
    0.0413
    0.9998
    0.9080
    0.0298
    0.0058
    0.0022
    0.9991
    0.9628
    0.9985
    0.9920
    0.9343
    0.0041
    0.0060
    0.0087
    0.8496
    0.9997
    0.9999
    0.9994
    0.9988
    0.9620
    0.0642
    0.9966
    0.8770
    0.9996
    0.0033
    0.9929
    0.0230
    0.8770
    0.4396
    0.0101
    0.0073
    0.0060
    0.0022
    0.0033
    0.9984
    0.0087
    0.0065
    0.8868
    0.9936
    0.9957
    1.0000
    1.0000
    0.0063
    0.7690
    0.9976
    0.0015
    0.0033
    0.9010
    0.9867
    0.0015
    0.9957
    0.6568
    0.0289
    0.0550
    0.9977
    0.9963
    0.0033
    1.0000
    0.0033
    0.9868
    0.9987
    0.0033
    0.0033
    0.9949
    0.0863
    0.0039
    0.0015
    0.8367
    0.0015
    0.0021
    0.4009
    0.9078
    0.9983
    0.0038
    0.7347
    0.9970
    0.0058
    0.0087
    0.9760
    0.0033
    0.0113
    0.9726
    0.0022
    0.8363
    0.9981
    0.9920
    0.0327
    0.0312
    0.9648
    0.9905
    0.0015
    0.9760
    0.0033
    0.0021
    0.9854
    0.9951
    0.0033
    0.0025
    0.0015
    0.9994
    0.0015
    0.0263
    0.0020
    0.1892
    0.9140
    0.0162
    0.0053
    0.3088
    0.9978
    0.0022
    0.0119
    0.2355
    1.0000
    0.9747
    0.9881
    1.0000
    0.9753
    0.0206
    0.0697
    0.0053
    0.0037
    1.0000
    0.9997
    0.0028
    0.0039
    0.0148
    0.0028
    0.0132
    0.0193
    0.0081
    0.0015
    0.0022
    0.0021
    0.0133
    0.0652
    0.0015
    0.9996
    0.0172
    0.0025
    0.0025
    0.0182
    0.9889
    0.0563
    0.0043
    0.0287
    0.0027
    0.9996
    0.0060
    0.0015
    0.0043
    0.0060
    0.0087
    0.0067
    0.0072
    0.0029
    0.9994
    0.0041
    0.0308
    0.0137
    0.0028
    0.0022
    0.0136
    0.0022
    0.0141
    0.0081
    0.0022
    0.0014
    0.9989
    0.0310
    0.9944
    0.1312
    0.9986
    0.0022
    0.0920
    0.0031
    0.0397
    0.9999
    0.0370
    0.0289
    0.0041
    1.0000
    0.0517
    0.9482
    0.0022
    0.0037
    0.0028
    0.0433
    0.0184
    0.0086
    0.9596
    0.9979
    0.9348
    0.0067
    0.0182
    0.0109
    0.9995
    0.0869
    0.0051
    0.0023
    0.0441
    0.0025
    0.0015
    0.0109
    0.0015
    0.9994
    0.0138
    0.0109
    0.0061
    0.9973
    0.0028
    0.6645
    0.9997
    0.9984
    0.0148
    0.0199
    0.0162
    0.0204
    0.0262
    0.0078
    0.0067
    0.9994
    0.9995
    0.9964
    0.0067
    0.0030
    0.0057
    0.0262
    0.0169
    0.0067
    0.0109
    0.0041
    0.0091
    0.0067
    0.0113
    0.9959
    0.0104
    0.0284
    1.0000
    0.9998
    0.0148
    0.0049
    0.0057
    1.0000
    0.3709
    0.2244
    0.0015
    0.9914
    0.0093
    0.9991
    0.4259
    0.0057
    0.0015
    0.0069
    0.0097
    0.0097
    0.0371
    0.0097
    0.0119
    0.0142
    0.0015
    0.0044
    0.9997
    0.0051
    0.0109
    0.0025
    0.0015
    0.0158
    0.0109
    0.0057
    0.9996
    0.9960
    0.0015
    0.0021
    0.0068
    0.9892
    0.0016
    0.0070
    0.9841
    0.9960
    0.0060
    0.0050
    0.0067
    0.0142
    0.0391
    0.0093
    0.9915
    0.0136
    0.0031
    0.0060
    0.0037
    0.0073
    0.0230
    0.0310
    0.0097
    0.0257
    0.0236
    0.0041
    0.0117
    0.0097
    0.0082
    0.0158
    0.9997
    0.0024
    0.0039
    0.9811
    0.0060
    0.0033
    0.0250
    0.0970
    0.0041
    0.0612
    0.0256
    0.0162
    0.0037
    0.0158
    0.0230
    0.0230
    0.0033
    0.0060
    0.0165
    0.9999
    0.0118
    0.0216
    0.9604
    0.9996
    0.9961
    0.9998
    0.0060
    0.0022
    0.9845
    0.0310
    0.0158
    0.0022
    0.0022
    0.0033
    0.0214
    0.9978
    0.9951
    0.0041
    0.0306
    0.0015
    1.0000
    0.0184
    0.9732
    0.0109
    0.8527
    0.9774
    0.9898
    0.0148
    0.9602
    0.0158
    0.0132
    0.0416
    0.0060
    0.0265
    0.0060
    0.0021
    0.0097
    0.9340
    0.9833
    0.9998
    0.0082
    0.0015
    0.9996
    0.0109
    0.9989
    0.9848
    1.0000
    0.0039
    0.0035
    0.0178
    0.0060
    0.0010
    0.0097
    0.0158
    0.0060
    0.3576
    0.0804
    0.0015
    0.0184
    0.0262
    0.9757
    0.0125
    0.0025
    0.0067
    0.0333
    0.0158
    0.0015
    0.9268
    0.0041
    0.0102
    0.9993
    0.0540
    0.0067
    0.0162
    0.0100
    0.0060
    0.0060
    0.0015
    0.0025
    0.0060
    0.0022
    0.0023
    0.9999
    0.0060
    0.0175
    0.0035
    0.0184
    0.0097
    0.0087
    0.0060
    0.0158
    0.2045
    0.9990
    0.0015
    0.0022
    0.0142
    0.0041
    0.0041
    0.0089
    0.0015
    0.0265
    0.0087
    0.7660
    0.9954
    0.9773
    0.0110
    0.0053
    0.0166
    0.0022
    0.0166
    0.0030
    0.0109
    0.0015
    0.0025
    1.0000
    0.9992
    0.0267
    0.0015
    0.0015
    0.0015
    0.0015
    0.0081
    0.0067
    0.0193
    0.0023
    0.9350
    0.0041
    0.0086
    0.0065
    0.0025
    0.9879
    0.9427
    0.9773

>> >> >> /usr/bin/gs: /opt/matlab/bin/glnxa64/libtiff.so.5: no version information available (required by /usr/lib/x86_64-linux-gnu/libgs.so.9)

ans =

     0

>> /usr/bin/gs: /opt/matlab/bin/glnxa64/libtiff.so.5: no version information available (required by /usr/lib/x86_64-linux-gnu/libgs.so.9)

ans =

     0

>> >> >> >> >> >> >> >> >> >> >> >> >> 
kernel_function =

polynomial

>> [Warning: The display option can only plot 2D training data.] 
[> In svmtrain (line 399)
  In svm_classifier_matrix (line 67)] 
[Warning: No figure was created during SVMTRAIN so the showplot argument will be
set false.] 
[> In svmclassify (line 122)
  In svm_classifier_matrix (line 71)] 

ans =

    0.4097

Elapsed time is 0.305988 seconds.

x_svm = 

  struct with fields:

          SupportVectors: [43×9 double]
                   Alpha: [43×1 double]
                    Bias: 4.1101
          KernelFunction: @poly_kernel
      KernelFunctionArgs: {}
              GroupNames: [350×1 logical]
    SupportVectorIndices: [43×1 double]
               ScaleData: [1×1 struct]
           FigureHandles: []

>> >> 
kernel_function =

quadratic

>> [Warning: The display option can only plot 2D training data.] 
[> In svmtrain (line 399)
  In svm_classifier_matrix (line 67)] 
[Warning: No figure was created during SVMTRAIN so the showplot argument will be
set false.] 
[> In svmclassify (line 122)
  In svm_classifier_matrix (line 71)] 

ans =

    0.9427

Elapsed time is 0.091976 seconds.

x_svm = 

  struct with fields:

          SupportVectors: [36×9 double]
                   Alpha: [36×1 double]
                    Bias: -0.0136
          KernelFunction: @quadratic_kernel
      KernelFunctionArgs: {}
              GroupNames: [350×1 logical]
    SupportVectorIndices: [36×1 double]
               ScaleData: [1×1 struct]
           FigureHandles: []

>> >> 
kernel_function =

rbf

>> [Warning: The display option can only plot 2D training data.] 
[> In svmtrain (line 399)
  In svm_classifier_matrix (line 67)] 
[Warning: No figure was created during SVMTRAIN so the showplot argument will be
set false.] 
[> In svmclassify (line 122)
  In svm_classifier_matrix (line 71)] 

ans =

    0.9685

Elapsed time is 0.060401 seconds.

x_svm = 

  struct with fields:

          SupportVectors: [149×9 double]
                   Alpha: [149×1 double]
                    Bias: 0.7985
          KernelFunction: @rbf_kernel
      KernelFunctionArgs: {}
              GroupNames: [350×1 logical]
    SupportVectorIndices: [149×1 double]
               ScaleData: [1×1 struct]
           FigureHandles: []

>> >> >> >> >> >> Replicate 1, 1 iterations, total sum of distances = 1703.
Replicate 2, 5 iterations, total sum of distances = 1703.
Replicate 3, 3 iterations, total sum of distances = 1703.
Replicate 4, 6 iterations, total sum of distances = 1703.
Replicate 5, 2 iterations, total sum of distances = 1703.
Replicate 6, 2 iterations, total sum of distances = 1703.
Replicate 7, 2 iterations, total sum of distances = 1709.
Replicate 8, 2 iterations, total sum of distances = 1709.
Replicate 9, 3 iterations, total sum of distances = 1709.
Replicate 10, 2 iterations, total sum of distances = 1709.
Best total sum of distances = 1703
Elapsed time is 1.655682 seconds.
>> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> gs: /opt/matlab/bin/glnxa64/libtiff.so.5: no version information available (required by /usr/lib/x86_64-linux-gnu/libgs.so.9)

ans =

     0

>> >> >> Elapsed time is 2.652193 seconds.
>> >> 